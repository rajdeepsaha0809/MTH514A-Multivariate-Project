```{r}
#Loading the Data

rm(list=ls())
set.seed(2022)
Data<- read.csv("https://raw.githubusercontent.com/rajdeepsaha0809/MTH514A-Multivariate-Project/main/diabetes.csv")
head(Data)
dim(Data)
str(Data)
```
```{r}
#Checking for missing values
sum(is.na(Data))
```
```{r}
#Check for Data Imbalance
attach(Data)
sum(Outcome == 1) / nrow(Data)
sum(Outcome == 0) / nrow(Data)
library(DMwR)
Data$Outcome <- as.factor(Outcome)
data <- SMOTE(Outcome~., Data, perc.over= 200, k= 5,  perc.under= 150)
attach(data)
sum(Outcome == 1)*100/nrow(data)
sum(Outcome == 0)*100/nrow(data)
```
```{r}
#Splitting the Data
names(data)
index= sample(1:nrow(data),floor(0.85*nrow(data)))
train= data[index, ]
remaining= data[-index, ]
index2 = sample(1:nrow(remaining),floor(2/3*nrow(remaining)))
crossval= remaining[index2, ]
test = remaining[-index2, ]
actual_diagnosis=crossval$diagnosis
dim(train)
dim(crossval)
dim(test)
```
```{r}
library(tibble)
library(cvms)
f_cfm <- function(x){
  cfm <- as.tibble(x)
  cname <- colnames(cfm)
  print(plot_confusion_matrix(cfm, target_col = cname[2], prediction_col =  cname[1], counts_col = cname[3]))
}
```

```{r}
#Logistic Regression
attach(data)
TenYearCHD <- as.factor(Outcome)
threshold <- seq(0.1, 0.9, 0.01)
fscore <- array(0)
for(i in 1:length(threshold)){
  logistic.fit <- glm(Outcome~., data = train, family = binomial)
  logistic.probs <- predict(logistic.fit, crossval, type = "response")
  logistic.pred <- rep(0, nrow(crossval))
  logistic.pred[logistic.probs > threshold[i]]= 1
  tab <- table(logistic.pred, crossval$Outcome)
  prec <- tab[2,2]/(tab[2,2] + tab[2,1])
  #print(prec)
  recall <- tab[2,2]/(tab[2,2] + tab[1,2])
  #print(recall)
  fscore[i] <- (2*prec*recall)/(prec + recall)
}
data.frame(threshold, fscore)
max_acc <- which.max(fscore)
paste("Maximum F1-score is for thresold value of ", threshold[max_acc], " and is = ",round(fscore[max_acc],4))

logistic.fit <- glm(Outcome~., data = crossval, family = binomial)
logistic.probs <- predict(logistic.fit, crossval, type = "response")
logistic.pred <- rep(0, nrow(crossval))
logistic.pred[logistic.probs > threshold[max_acc]] = 1
actual <- crossval$Outcome
logistic_table <- table(logistic.pred, actual)
f_cfm(logistic_table)
```
```{r}
names <- colnames(data)
library(ggplot2)
n <- ncol(data)-1
for(i in 1:n){
  ggplot(data[i], aes(x= names[i]))+
  geom_density(color="darkblue", fill="lightblue")
}
decision <- rep("Reject Normality", n)
pval <- array(0)
for(i in 1:n){
  decision[shapiro.test(data[,i])$p.value>0.5]="Accept Normality"
  pval[i] <- shapiro.test(data[,i])$p.value
}
```

```{r}
#Decision Tree
library(tree)
tree.fit <- tree(Outcome~., data= train)
cv.outcome <- cv.tree(tree.fit, FUN= prune.misclass)
cv.outcome #dev corresponds to misclassification error rate
par(mfrow=c(1,2))
plot(cv.outcome$size, cv.outcome$dev, type= "b")
plot(cv.outcome$k, cv.outcome$dev, type= "b")
prune.tree <- prune.misclass(tree.fit, best = 8)
plot(prune.tree)
text(prune.tree, pretty = 0)
tree.pred <- predict(prune.tree, crossval, type = "class")
tree_table <- table(tree.pred, actual)
f_cfm(tree_table)
```
```{r}
#Random Forest
library(randomForest)
used_pred <- floor(sqrt(ncol(data)))
rf.fit <- randomForest(Outcome~., data = train, mtry = used_pred,
           importance = TRUE, maxdepth = 8)
rf.pred <- predict(rf.fit, newdata = crossval)
rf_table <- table(rf.pred, actual)
f_cfm(rf_table)
```
```{r}
#k-NN
library(class)
train.X <- train[,2:ncol(data)]
crossval.X <- crossval[,2:ncol(data)]
train.outcome <- train$Outcome
k <- 5:10
misclass <- array(0)
for (i in 1:length(k)){
  knn.pred <- knn(train.X, crossval.X, train.outcome, k= i)
  misclass[i] <- mean(knn.pred!=crossval$Outcome)
}
plot(k, misclass)
paste("The choice of k is", "k=",k[which.min(misclass)])
knn.pred <- knn(train.X, crossval.X, train.outcome, k= 5)
knn_table <- table(knn.pred, actual)
f_cfm(knn_table)
```
```{r}
F_score <- function(x){
  p <- x[1,1]/(x[1,1]+ x[1,2])
  r <- x[1,1]/(x[1,1]+ x[2,1])
  f <- round(2*p*r/(p+r), 4)
  f
}
F_score(logistic_table)
F_score(tree_table)
F_score(rf_table)
F_score(knn_table)
paste("We have got the maximum F-Score for Random Forest which is", round(F_score(rf_table), 4))
```
```{r}
#Plotting ROC curve

#Logistic Regression
library(pROC)
logistic_prob = predict(logistic.fit, newdata = crossval, type = "response")
logistic_roc = roc(crossval$Outcome ~ logistic_prob, plot = TRUE, print.auc = TRUE)
paste("Area under the curve is",round(auc(logistic_roc), 4))

#Decision Tree
tree_predict= predict(prune.tree, crossval, type="vector")
tree_roc = roc(crossval$Outcome~tree_predict[,2], plot = TRUE, print.auc = TRUE)
paste("Area under the curve is",round(auc(tree_roc), 4))

#Random Forest
rf_predict= predict(rf.fit, crossval, type="prob")
rf_roc = roc(crossval$Outcome~rf_predict[,2], plot = TRUE, print.auc = TRUE)
paste("Area under the curve is",round(auc(rf_roc), 4))

#k-NN
knn.pred <- knn(train.X, crossval.X, train.outcome, k= 5, prob= TRUE)
prob <- attr(knn.pred, "prob")
knn_roc = roc(crossval$Outcome~prob, plot = TRUE, print.auc = TRUE)
paste("Area under the curve is",round(auc(knn_roc), 4))
```
